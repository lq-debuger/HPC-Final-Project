n=100,dt=0.000020,dx=0.010000,steps=100000,alpha=0.200000,beta=0.600000
Do not read u_old from uold.h5 ...
Vec Object: 4 MPI processes
  type: mpi
Process [0]
0.
0.0314108
0.0627905
0.0941083
0.125333
0.156434
0.187381
0.218143
0.24869
0.278991
0.309017
0.338738
0.368125
0.397148
0.425779
0.45399
0.481754
0.509041
0.535827
0.562083
0.587785
0.612907
0.637424
0.661312
0.684547
Process [1]
0.707107
0.728969
0.750111
0.770513
0.790155
0.809017
0.827081
0.844328
0.860742
0.876307
0.891007
0.904827
0.917755
0.929776
0.940881
0.951057
0.960294
0.968583
0.975917
0.982287
0.987688
0.992115
0.995562
0.998027
0.999507
Process [2]
1.
0.999507
0.998027
0.995562
0.992115
0.987688
0.982287
0.975917
0.968583
0.960294
0.951057
0.940881
0.929776
0.917755
0.904827
0.891007
0.876307
0.860742
0.844328
0.827081
0.809017
0.790155
0.770513
0.750111
0.728969
Process [3]
0.707107
0.684547
0.661312
0.637424
0.612907
0.587785
0.562083
0.535827
0.509041
0.481754
0.45399
0.425779
0.397148
0.368125
0.338738
0.309017
0.278991
0.24869
0.218143
0.187381
0.156434
0.125333
0.0941083
0.0627905
0.
It's explicit scheme
step=100000
---------------------
Vec Object: 4 MPI processes
  type: mpi
Process [0]
0.00315132
0.00630265
0.00945083
0.0125927
0.0157252
0.0188452
0.0219495
0.0250351
0.0280988
0.0311377
0.0341487
0.0371288
0.040075
0.0429845
0.0458542
0.0486813
0.051463
0.0541965
0.0568792
0.0595082
0.0620811
0.0645951
0.0670479
0.0694369
0.0717598
Process [1]
0.0740143
0.076198
0.0783089
0.0803447
0.0823035
0.0841832
0.0859821
0.0876982
0.08933
0.0908756
0.0923336
0.0937025
0.094981
0.0961676
0.0972613
0.0982609
0.0991654
0.0999738
0.100685
0.101299
0.101815
0.102232
0.10255
0.102768
0.102887
Process [2]
0.102905
0.102824
0.102642
0.102361
0.10198
0.1015
0.100921
0.100244
0.0994696
0.0985981
0.0976306
0.096568
0.0954113
0.0941616
0.0928202
0.0913882
0.0898672
0.0882585
0.0865638
0.0847846
0.0829227
0.0809799
0.0789581
0.0768593
0.0746854
Process [3]
0.0724386
0.0701211
0.0677352
0.0652832
0.0627674
0.0601903
0.0575544
0.0548623
0.0521166
0.0493201
0.0464753
0.0435852
0.0406525
0.03768
0.0346708
0.0316277
0.0285537
0.0254517
0.0223249
0.0191763
0.016009
0.012826
0.00963049
0.00642556
0.00321435
The program is finished
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./final.out on a  named r01n12 with 4 processors, by mae-liq1 Mon Jun  6 19:57:50 2022
Using Petsc Release Version 3.16.6, Mar 30, 2022 

                         Max       Max/Min     Avg       Total
Time (sec):           2.305e+02     1.000   2.305e+02
Objects:              1.400e+01     1.000   1.400e+01
Flop:                 2.250e+07     1.009   2.240e+07  8.960e+07
Flop/sec:             9.763e+04     1.009   9.720e+04  3.888e+05
MPI Messages:         2.000e+05     2.000   1.500e+05  6.000e+05
MPI Message Lengths:  1.601e+06     1.999   8.005e+00  4.803e+06
MPI Reductions:       1.000e+05     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 2.3045e+02 100.0%  8.9599e+07 100.0%  6.000e+05 100.0%  8.005e+00      100.0%  1.000e+05 100.0%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          4 1.0 2.8288e-02 1.7 0.00e+00 0.0 1.2e+01 8.0e+00 4.0e+00  0  0  0  0  0   0  0  0  0  0     0
BuildTwoSidedF         3 1.0 2.7654e-02 2.3 0.00e+00 0.0 1.8e+01 1.0e+02 3.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecView            10001 1.0 2.1780e+02 1.1 0.00e+00 0.0 6.0e+00 2.0e+02 0.0e+00 91  0  0  0  0  91  0  0  0  0     0
VecNorm            99999 1.0 1.5923e+01 1.8 5.00e+06 1.0 0.0e+00 0.0e+00 1.0e+05  5 22  0  0100   5 22  0  0100     1
VecCopy            99999 1.0 1.5831e-02 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                 1 1.0 2.0027e-05 3.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY            99999 1.0 5.1429e-02 1.5 5.00e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0 22  0  0  0   0 22  0  0  0   389
VecAssemblyBegin       2 1.0 1.9828e-02 3.2 0.00e+00 0.0 1.8e+01 1.0e+02 2.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAssemblyEnd         2 1.0 4.2915e-05 9.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecScatterBegin    99999 1.0 2.8740e-01 2.1 0.00e+00 0.0 6.0e+05 8.0e+00 1.0e+00  0  0100100  0   0  0100100  0     0
VecScatterEnd      99999 1.0 1.9895e+0116.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  4  0  0  0  0   4  0  0  0  0     0
MatMult            99999 1.0 2.0172e+0114.4 1.25e+07 1.0 6.0e+05 8.0e+00 1.0e+00  4 55100100  0   4 55100100  0     2
MatAssemblyBegin       1 1.0 1.0816e-02 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd         1 1.0 3.1382e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 5.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetGraph             1 1.0 9.5367e-07 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetUp                1 1.0 1.1316e-02 1.2 0.00e+00 0.0 1.2e+01 4.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFPack             99999 1.0 2.7443e-02 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFUnpack           99999 1.0 8.8480e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Vector     5              5         9240     0.
              Matrix     3              3        13644     0.
           Index Set     2              2         1788     0.
   Star Forest Graph     1              1         1200     0.
              Viewer     3              2         1648     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 0.000875187
Average time for zero size MPI_Send(): 8.52346e-06
#PETSc Option Table entries:
-log_view
-n 100
-restart 0
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --prefix=/work/mae-liq1/lib/petsc-3.16.6 --with-blaslapack-dir=/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mkl --with-debugging=no --download-hypre --download-metis --download-hdf5=/work/mae-liq1/software/hdf5-1.12.1.tar.gz --with-mpi-dir=/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64 COPTFLAGS="-O3 -march=native -mtune=native" CXXPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native"
-----------------------------------------
Libraries compiled on 2022-05-03 14:18:39 on login03 
Machine characteristics: Linux-3.10.0-862.el7.x86_64-x86_64-with-redhat-7.5-Maipo
Using PETSc directory: /work/mae-liq1/lib/petsc-3.16.6
Using PETSc arch: 
-----------------------------------------

Using C compiler: /share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -O3 -march=native -mtune=native  -std=c99 
Using Fortran compiler: /share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native     -std=c99
-----------------------------------------

Using include paths: -I/work/mae-liq1/lib/petsc-3.16.6/include -I/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mkl/include -I/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/include
-----------------------------------------

Using C linker: /share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/bin/mpicc
Using Fortran linker: /share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/bin/mpif90
Using libraries: -Wl,-rpath,/work/mae-liq1/lib/petsc-3.16.6/lib -L/work/mae-liq1/lib/petsc-3.16.6/lib -lpetsc -Wl,-rpath,/work/mae-liq1/lib/petsc-3.16.6/lib -L/work/mae-liq1/lib/petsc-3.16.6/lib -Wl,-rpath,/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mkl/lib/intel64 -L/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mkl/lib/intel64 -Wl,-rpath,/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib/release_mt -L/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib/release_mt -Wl,-rpath,/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib -L/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib -Wl,-rpath,/usr/lib/gcc/x86_64-redhat-linux/4.8.5 -L/usr/lib/gcc/x86_64-redhat-linux/4.8.5 -Wl,-rpath,/opt/intel/mpi-rt/2017.0.0/intel64/lib/release_mt -Wl,-rpath,/opt/intel/mpi-rt/2017.0.0/intel64/lib -lHYPRE -lmkl_intel_lp64 -lmkl_core -lmkl_sequential -lpthread -lhdf5_hl -lhdf5 -lmetis -lm -lX11 -lstdc++ -ldl -lmpifort -lmpi -lmpigi -lrt -lpthread -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lstdc++ -ldl
-----------------------------------------

