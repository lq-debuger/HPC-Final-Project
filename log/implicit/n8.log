n=100,dt=0.000020,dx=0.010000,steps=100000,alpha=-0.200000,beta=1.400000
Do not read u_old from uold.h5 ...
inital u is : 
Vec Object: 8 MPI processes
  type: mpi
Process [0]
0.
0.0314108
0.0627905
0.0941083
0.125333
0.156434
0.187381
0.218143
0.24869
0.278991
0.309017
0.338738
0.368125
Process [1]
0.397148
0.425779
0.45399
0.481754
0.509041
0.535827
0.562083
0.587785
0.612907
0.637424
0.661312
0.684547
0.707107
Process [2]
0.728969
0.750111
0.770513
0.790155
0.809017
0.827081
0.844328
0.860742
0.876307
0.891007
0.904827
0.917755
0.929776
Process [3]
0.940881
0.951057
0.960294
0.968583
0.975917
0.982287
0.987688
0.992115
0.995562
0.998027
0.999507
1.
0.999507
Process [4]
0.998027
0.995562
0.992115
0.987688
0.982287
0.975917
0.968583
0.960294
0.951057
0.940881
0.929776
0.917755
Process [5]
0.904827
0.891007
0.876307
0.860742
0.844328
0.827081
0.809017
0.790155
0.770513
0.750111
0.728969
0.707107
Process [6]
0.684547
0.661312
0.637424
0.612907
0.587785
0.562083
0.535827
0.509041
0.481754
0.45399
0.425779
0.397148
Process [7]
0.368125
0.338738
0.309017
0.278991
0.24869
0.218143
0.187381
0.156434
0.125333
0.0941083
0.0627905
0.
exact solution is : 
Vec Object: 8 MPI processes
  type: mpi
Process [0]
0.
0.00318258
0.00636201
0.00953517
0.0126989
0.0158501
0.0189857
0.0221025
0.0251976
0.0282677
0.03131
0.0343213
0.0372988
Process [1]
0.0402395
0.0431405
0.0459989
0.0488119
0.0515767
0.0542906
0.056951
0.0595551
0.0621005
0.0645846
0.0670049
0.0693591
0.0716449
Process [2]
0.07386
0.0760021
0.0780693
0.0800594
0.0819706
0.0838008
0.0855483
0.0872114
0.0887884
0.0902778
0.0916781
0.092988
0.0942061
Process [3]
0.0953312
0.0963622
0.0972981
0.098138
0.098881
0.0995265
0.100074
0.100522
0.100872
0.101121
0.101271
0.101321
0.101271
Process [4]
0.101121
0.100872
0.100522
0.100074
0.0995265
0.098881
0.098138
0.0972981
0.0963622
0.0953312
0.0942061
0.092988
Process [5]
0.0916781
0.0902778
0.0887884
0.0872114
0.0855483
0.0838008
0.0819706
0.0800594
0.0780693
0.0760021
0.07386
0.0716449
Process [6]
0.0693591
0.0670049
0.0645846
0.0621005
0.0595551
0.056951
0.0542906
0.0515767
0.0488119
0.0459989
0.0431405
0.0402395
Process [7]
0.0372988
0.0343213
0.03131
0.0282677
0.0251976
0.0221025
0.0189857
0.0158501
0.0126989
0.00953517
0.00636201
0.00318258
It's implicit scheme
step=100000
---------------------
numerical result u is : 
Vec Object: 8 MPI processes
  type: mpi
Process [0]
0.00315132
0.0063026
0.00945064
0.0125924
0.0157248
0.0188447
0.0219489
0.0250344
0.0280981
0.031137
0.0341479
0.037128
0.0400741
Process [1]
0.0429835
0.0458532
0.0486803
0.051462
0.0541955
0.0568781
0.0595072
0.06208
0.0645941
0.0670469
0.069436
0.0717589
0.0740134
Process [2]
0.0761971
0.078308
0.0803439
0.0823027
0.0841825
0.0859814
0.0876976
0.0893294
0.090875
0.0923331
0.0937021
0.0949805
0.0961673
Process [3]
0.097261
0.0982606
0.0991652
0.0999737
0.100685
0.101299
0.101815
0.102232
0.10255
0.102768
0.102887
0.102905
0.102824
Process [4]
0.102642
0.102361
0.10198
0.1015
0.100922
0.100245
0.0994701
0.0985987
0.0976312
0.0965686
0.0954119
0.0941622
Process [5]
0.0928208
0.0913889
0.0898678
0.0882592
0.0865644
0.0847853
0.0829234
0.0809806
0.0789588
0.0768599
0.0746861
0.0724393
Process [6]
0.0701218
0.0677359
0.0652838
0.062768
0.0601909
0.057555
0.0548629
0.0521172
0.0493206
0.0464758
0.0435857
0.0406529
Process [7]
0.0376805
0.0346712
0.031628
0.028554
0.025452
0.0223252
0.0191766
0.0160092
0.0128262
0.0096306
0.00642563
0.00321439
The program is finished
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./final.out on a  named r01n12 with 8 processors, by mae-liq1 Mon Jun  6 20:23:41 2022
Using Petsc Release Version 3.16.6, Mar 30, 2022 

                         Max       Max/Min     Avg       Total
Time (sec):           5.417e+02     1.000   5.417e+02
Objects:              3.800e+01     1.000   3.800e+01
Flop:                 8.261e+07     1.092   7.927e+07  6.342e+08
Flop/sec:             1.525e+05     1.092   1.463e+05  1.171e+06
MPI Messages:         5.737e+05     2.000   5.020e+05  4.016e+06
MPI Message Lengths:  4.590e+06     2.000   8.001e+00  3.213e+07
MPI Reductions:       7.737e+05     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 5.4168e+02 100.0%  6.3417e+08 100.0%  4.016e+06 100.0%  8.001e+00      100.0%  7.737e+05 100.0%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          5 1.0 1.5766e-01 1.9 0.00e+00 0.0 3.5e+01 8.8e+00 5.0e+00  0  0  0  0  0   0  0  0  0  0     0
BuildTwoSidedF         4 1.0 1.5424e-01 1.9 0.00e+00 0.0 6.3e+01 5.4e+01 4.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecView            10002 1.0 4.6643e+02 1.1 0.00e+00 0.0 2.1e+01 1.0e+02 0.0e+00 81  0  0  0  0  81  0  0  0  0     0
VecMDot           286855 1.0 3.6588e+01 1.3 1.40e+07 1.1 0.0e+00 0.0e+00 2.9e+05  6 17  0  0 37   6 17  0  0 37     3
VecNorm           486853 1.0 6.5633e+01 2.5 1.27e+07 1.1 0.0e+00 0.0e+00 4.9e+05 10 15  0  0 63  10 15  0  0 63     1
VecScale          386854 1.0 1.0834e-01 1.5 5.03e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  6  0  0  0   0  6  0  0  0   357
VecCopy           199998 1.0 2.9628e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet            199999 1.0 3.5435e-02 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY           199998 1.0 3.7399e-01 1.0 5.20e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  6  0  0  0   0  6  0  0  0   107
VecMAXPY          386854 1.0 5.7683e-02 1.1 2.20e+07 1.1 0.0e+00 0.0e+00 0.0e+00  0 27  0  0  0   0 27  0  0  0  2939
VecAssemblyBegin       3 1.0 1.1607e-01 2.9 0.00e+00 0.0 6.3e+01 5.4e+01 3.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAssemblyEnd         3 1.0 6.5106e-025936.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecPointwiseMult  386854 1.0 4.7976e-02 1.1 5.03e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  6  0  0  0   0  6  0  0  0   806
VecScatterBegin   286855 1.0 2.9489e+00 6.8 0.00e+00 0.0 4.0e+06 8.0e+00 1.0e+00  0  0100100  0   0  0100100  0     0
VecScatterEnd     286855 1.0 1.2331e+01 2.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0
VecNormalize      386854 1.0 6.3433e+01 2.6 1.51e+07 1.1 0.0e+00 0.0e+00 3.9e+05 10 18  0  0 50  10 18  0  0 50     2
MatMult           286855 1.0 1.2919e+01 2.0 1.86e+07 1.1 4.0e+06 8.0e+00 1.0e+00  2 22100100  0   2 22100100  0    11
MatAssemblyBegin       1 1.0 4.3114e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd         1 1.0 6.3945e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 5.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetGraph             1 1.0 1.1921e-06 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetUp                1 1.0 8.2800e-03 5.6 0.00e+00 0.0 2.8e+01 4.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFPack            286855 1.0 6.2841e-02 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFUnpack          286855 1.0 2.1914e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSetUp               1 1.0 6.6996e-05 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve           99999 1.0 1.0847e+02 1.5 7.74e+07 1.1 4.0e+06 8.0e+00 6.7e+05 18 94100100 87  18 94100100 87     5
KSPGMRESOrthog    286855 1.0 3.6727e+01 1.3 2.86e+07 1.1 0.0e+00 0.0e+00 2.9e+05  6 35  0  0 37   6 35  0  0 37     6
PCSetUp                1 1.0 5.0068e-06 2.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
PCApply           386854 1.0 1.3409e-01 1.1 5.03e+06 1.1 0.0e+00 0.0e+00 2.0e+00  0  6  0  0  0   0  6  0  0  0   289
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Vector    22             21        38648     0.
              Matrix     3              3        11724     0.
           Index Set     2              2         1788     0.
   Star Forest Graph     3              3         3312     0.
              Viewer     3              2         1648     0.
       Krylov Solver     1              1        18848     0.
      Preconditioner     1              1          872     0.
    Distributed Mesh     1              1         5048     0.
     Discrete System     1              1          896     0.
           Weak Form     1              1          616     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 1.72138e-05
Average time for zero size MPI_Send(): 8.32379e-05
#PETSc Option Table entries:
-log_view
-n 100
-restart 0
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --prefix=/work/mae-liq1/lib/petsc-3.16.6 --with-blaslapack-dir=/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mkl --with-debugging=no --download-hypre --download-metis --download-hdf5=/work/mae-liq1/software/hdf5-1.12.1.tar.gz --with-mpi-dir=/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64 COPTFLAGS="-O3 -march=native -mtune=native" CXXPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native"
-----------------------------------------
Libraries compiled on 2022-05-03 14:18:39 on login03 
Machine characteristics: Linux-3.10.0-862.el7.x86_64-x86_64-with-redhat-7.5-Maipo
Using PETSc directory: /work/mae-liq1/lib/petsc-3.16.6
Using PETSc arch: 
-----------------------------------------

Using C compiler: /share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -O3 -march=native -mtune=native  -std=c99 
Using Fortran compiler: /share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native     -std=c99
-----------------------------------------

Using include paths: -I/work/mae-liq1/lib/petsc-3.16.6/include -I/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mkl/include -I/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/include
-----------------------------------------

Using C linker: /share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/bin/mpicc
Using Fortran linker: /share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/bin/mpif90
Using libraries: -Wl,-rpath,/work/mae-liq1/lib/petsc-3.16.6/lib -L/work/mae-liq1/lib/petsc-3.16.6/lib -lpetsc -Wl,-rpath,/work/mae-liq1/lib/petsc-3.16.6/lib -L/work/mae-liq1/lib/petsc-3.16.6/lib -Wl,-rpath,/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mkl/lib/intel64 -L/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mkl/lib/intel64 -Wl,-rpath,/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib/release_mt -L/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib/release_mt -Wl,-rpath,/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib -L/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib -Wl,-rpath,/usr/lib/gcc/x86_64-redhat-linux/4.8.5 -L/usr/lib/gcc/x86_64-redhat-linux/4.8.5 -Wl,-rpath,/opt/intel/mpi-rt/2017.0.0/intel64/lib/release_mt -Wl,-rpath,/opt/intel/mpi-rt/2017.0.0/intel64/lib -lHYPRE -lmkl_intel_lp64 -lmkl_core -lmkl_sequential -lpthread -lhdf5_hl -lhdf5 -lmetis -lm -lX11 -lstdc++ -ldl -lmpifort -lmpi -lmpigi -lrt -lpthread -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lstdc++ -ldl
-----------------------------------------

