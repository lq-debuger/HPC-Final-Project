n=100,dt=0.000020,dx=0.010000,steps=1000000,alpha=-0.200000,beta=1.400000
Do not read u_old from uold.h5 ...
Vec Object: 1 MPI processes
  type: seq
0.
1.01005
1.0202
1.03045
1.04081
1.05127
1.06184
1.07251
1.08329
1.09417
1.10517
1.11628
1.1275
1.13883
1.15027
1.16183
1.17351
1.1853
1.19722
1.20925
1.2214
1.23368
1.24608
1.2586
1.27125
1.28403
1.29693
1.30996
1.32313
1.33643
1.34986
1.36343
1.37713
1.39097
1.40495
1.41907
1.43333
1.44773
1.46228
1.47698
1.49182
1.50682
1.52196
1.53726
1.55271
1.56831
1.58407
1.59999
1.61607
1.63232
1.64872
1.66529
1.68203
1.69893
1.71601
1.73325
1.75067
1.76827
1.78604
1.80399
1.82212
1.84043
1.85893
1.87761
1.89648
1.91554
1.93479
1.95424
1.97388
1.99372
2.01375
2.03399
2.05443
2.07508
2.09594
2.117
2.13828
2.15977
2.18147
2.2034
2.22554
2.24791
2.2705
2.29332
2.31637
2.33965
2.36316
2.38691
2.4109
2.43513
2.4596
2.48432
2.50929
2.53451
2.55998
2.58571
2.6117
2.63794
2.66446
0.
It's implicit scheme
Vec Object: 1 MPI processes
  type: seq
1.71818
1.71826
1.71848
1.71885
1.71937
1.72002
1.72081
1.72173
1.72277
1.72395
1.72524
1.72665
1.72817
1.72981
1.73155
1.73338
1.73532
1.73735
1.73947
1.74167
1.74396
1.74632
1.74875
1.75125
1.75381
1.75644
1.75912
1.76185
1.76462
1.76744
1.7703
1.7732
1.77612
1.77907
1.78204
1.78503
1.78804
1.79105
1.79407
1.7971
1.80012
1.80314
1.80615
1.80915
1.81214
1.8151
1.81804
1.82096
1.82385
1.82671
1.82954
1.83232
1.83507
1.83778
1.84044
1.84305
1.84562
1.84813
1.85058
1.85299
1.85533
1.85761
1.85984
1.862
1.86409
1.86613
1.86809
1.86999
1.87182
1.87358
1.87527
1.87689
1.87844
1.87992
1.88133
1.88267
1.88394
1.88514
1.88627
1.88733
1.88832
1.88925
1.89011
1.8909
1.89164
1.89231
1.89291
1.89346
1.89396
1.8944
1.89478
1.89511
1.8954
1.89563
1.89583
1.89598
1.89609
1.89617
1.89621
1.89623
step=1000000
---------------------
Vec Object: 1 MPI processes
  type: seq
14.5316
14.5316
14.5317
14.5318
14.532
14.5323
14.5326
14.5329
14.5333
14.5338
14.5342
14.5348
14.5353
14.5359
14.5364
14.537
14.5377
14.5383
14.539
14.5396
14.5403
14.541
14.5417
14.5424
14.5431
14.5437
14.5444
14.5451
14.5457
14.5464
14.547
14.5476
14.5482
14.5488
14.5493
14.5498
14.5503
14.5508
14.5512
14.5517
14.5521
14.5524
14.5527
14.553
14.5533
14.5535
14.5537
14.5539
14.554
14.5541
14.5541
14.5541
14.5541
14.5541
14.554
14.5538
14.5537
14.5535
14.5532
14.553
14.5527
14.5524
14.552
14.5516
14.5512
14.5508
14.5503
14.5498
14.5493
14.5488
14.5482
14.5477
14.5471
14.5465
14.5459
14.5453
14.5447
14.5441
14.5434
14.5428
14.5422
14.5416
14.541
14.5404
14.5398
14.5392
14.5387
14.5382
14.5377
14.5372
14.5368
14.5364
14.536
14.5357
14.5354
14.5351
14.5349
14.5348
14.5347
14.5347
The program is finished
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./final.out on a  named r01n12 with 1 processor, by mae-liq1 Mon Jun  6 17:05:51 2022
Using Petsc Release Version 3.16.6, Mar 30, 2022 

                         Max       Max/Min     Avg       Total
Time (sec):           3.713e+02     1.000   3.713e+02
Objects:              3.000e+01     1.000   3.000e+01
Flop:                 4.386e+09     1.000   4.386e+09  4.386e+09
Flop/sec:             1.181e+07     1.000   1.181e+07  1.181e+07
MPI Messages:         0.000e+00     0.000   0.000e+00  0.000e+00
MPI Message Lengths:  0.000e+00     0.000   0.000e+00  0.000e+00
MPI Reductions:       0.000e+00     0.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.7130e+02 100.0%  4.3861e+09 100.0%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

VecView           100002 1.0 3.6100e+02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00 97  0  0  0  0  97  0  0  0  0     0
VecMDot          2000423 1.0 3.6451e-01 1.0 5.97e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0 14  0  0  0   0 14  0  0  0  1639
VecNorm          4000421 1.0 6.5086e-01 1.0 7.96e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0 18  0  0  0   0 18  0  0  0  1223
VecScale         3000422 1.0 5.0225e-01 1.0 3.00e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0  7  0  0  0   0  7  0  0  0   597
VecCopy          1999998 1.0 3.0796e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet           2000017 1.0 3.0815e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY          1999998 1.0 4.9187e-01 1.0 4.00e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0  9  0  0  0   0  9  0  0  0   813
VecMAXPY         3000422 1.0 5.7166e-01 1.0 1.00e+09 1.0 0.0e+00 0.0e+00 0.0e+00  0 23  0  0  0   0 23  0  0  0  1750
VecAssemblyBegin       2 1.0 0.0000e+00 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAssemblyEnd         2 1.0 0.0000e+00 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecPointwiseMult 3000422 1.0 4.8656e-01 1.0 3.00e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0  7  0  0  0   0  7  0  0  0   617
VecNormalize     3000422 1.0 1.7250e+00 1.0 8.97e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0 20  0  0  0   0 20  0  0  0   520
MatMult          2000423 1.0 9.9552e-01 1.0 9.92e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0 23  0  0  0   0 23  0  0  0   997
MatAssemblyBegin       1 1.0 0.0000e+00 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd         1 1.0 5.3167e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSetUp               1 1.0 6.9857e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve          999999 1.0 8.7297e+00 1.0 3.99e+09 1.0 0.0e+00 0.0e+00 0.0e+00  2 91  0  0  0   2 91  0  0  0   457
KSPGMRESOrthog   2000423 1.0 1.3550e+00 1.0 1.20e+09 1.0 0.0e+00 0.0e+00 0.0e+00  0 27  0  0  0   0 27  0  0  0   884
PCSetUp                1 1.0 5.0068e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
PCApply          3000422 1.0 9.6474e-01 1.0 3.00e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0  7  0  0  0   0  7  0  0  0   311
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Vector    19             19        46056     0.
              Matrix     1              1        11276     0.
              Viewer     3              2         1648     0.
       Krylov Solver     1              1        18848     0.
      Preconditioner     1              1          872     0.
    Distributed Mesh     1              1         5048     0.
   Star Forest Graph     2              2         2112     0.
     Discrete System     1              1          896     0.
           Weak Form     1              1          616     0.
========================================================================================================================
Average time to get PetscTime(): 0.
#PETSc Option Table entries:
-log_view
-n 100
-restart 0
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --prefix=/work/mae-liq1/lib/petsc-3.16.6 --with-blaslapack-dir=/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mkl --with-debugging=no --download-hypre --download-metis --download-hdf5=/work/mae-liq1/software/hdf5-1.12.1.tar.gz --with-mpi-dir=/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64 COPTFLAGS="-O3 -march=native -mtune=native" CXXPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native"
-----------------------------------------
Libraries compiled on 2022-05-03 14:18:39 on login03 
Machine characteristics: Linux-3.10.0-862.el7.x86_64-x86_64-with-redhat-7.5-Maipo
Using PETSc directory: /work/mae-liq1/lib/petsc-3.16.6
Using PETSc arch: 
-----------------------------------------

Using C compiler: /share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -O3 -march=native -mtune=native  -std=c99 
Using Fortran compiler: /share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native     -std=c99
-----------------------------------------

Using include paths: -I/work/mae-liq1/lib/petsc-3.16.6/include -I/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mkl/include -I/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/include
-----------------------------------------

Using C linker: /share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/bin/mpicc
Using Fortran linker: /share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/bin/mpif90
Using libraries: -Wl,-rpath,/work/mae-liq1/lib/petsc-3.16.6/lib -L/work/mae-liq1/lib/petsc-3.16.6/lib -lpetsc -Wl,-rpath,/work/mae-liq1/lib/petsc-3.16.6/lib -L/work/mae-liq1/lib/petsc-3.16.6/lib -Wl,-rpath,/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mkl/lib/intel64 -L/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mkl/lib/intel64 -Wl,-rpath,/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib/release_mt -L/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib/release_mt -Wl,-rpath,/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib -L/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib -Wl,-rpath,/usr/lib/gcc/x86_64-redhat-linux/4.8.5 -L/usr/lib/gcc/x86_64-redhat-linux/4.8.5 -Wl,-rpath,/opt/intel/mpi-rt/2017.0.0/intel64/lib/release_mt -Wl,-rpath,/opt/intel/mpi-rt/2017.0.0/intel64/lib -lHYPRE -lmkl_intel_lp64 -lmkl_core -lmkl_sequential -lpthread -lhdf5_hl -lhdf5 -lmetis -lm -lX11 -lstdc++ -ldl -lmpifort -lmpi -lmpigi -lrt -lpthread -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lstdc++ -ldl
-----------------------------------------

